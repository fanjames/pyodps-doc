# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2014-2018, The Alibaba Group Holding Ltd.
# This file is distributed under the same license as the PyODPS package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2018.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PyODPS 0.7.16\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2018-05-11 09:42+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.5.3\n"

#: ../../source/df-sort-distinct-apply.rst:4
msgid "排序、去重、采样、数据变换"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:9
msgid "from odps.df import DataFrame"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:13
msgid "iris = DataFrame(o.get_table('pyodps_iris'))"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:15
msgid "排序"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:17
msgid "排序操作只能作用于Collection。我们只需要调用sort或者sort\\_values方法。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:28
msgid ""
">>> iris.sort('sepalwidth').head(5)\n"
"   sepallength  sepalwidth  petallength  petalwidth             name\n"
"0          5.0         2.0          3.5         1.0  Iris-versicolor\n"
"1          6.2         2.2          4.5         1.5  Iris-versicolor\n"
"2          6.0         2.2          5.0         1.5   Iris-virginica\n"
"3          6.0         2.2          4.0         1.0  Iris-versicolor\n"
"4          5.5         2.3          4.0         1.3  Iris-versicolor"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:29
msgid "如果想要降序排列，则可以使用参数\\ ``ascending``\\ ，并设为False。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:40
msgid ""
">>> iris.sort('sepalwidth', ascending=False).head(5)\n"
"   sepallength  sepalwidth  petallength  petalwidth         name\n"
"0          5.7         4.4          1.5         0.4  Iris-setosa\n"
"1          5.5         4.2          1.4         0.2  Iris-setosa\n"
"2          5.2         4.1          1.5         0.1  Iris-setosa\n"
"3          5.8         4.0          1.2         0.2  Iris-setosa\n"
"4          5.4         3.9          1.3         0.4  Iris-setosa"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:41
msgid "也可以这样调用，来进行降序排列："
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:52
msgid ""
">>> iris.sort(-iris.sepalwidth).head(5)\n"
"   sepallength  sepalwidth  petallength  petalwidth         name\n"
"0          5.7         4.4          1.5         0.4  Iris-setosa\n"
"1          5.5         4.2          1.4         0.2  Iris-setosa\n"
"2          5.2         4.1          1.5         0.1  Iris-setosa\n"
"3          5.8         4.0          1.2         0.2  Iris-setosa\n"
"4          5.4         3.9          1.3         0.4  Iris-setosa"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:53
msgid "多字段排序也很简单："
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:64
msgid ""
">>> iris.sort(['sepalwidth', 'petallength']).head(5)\n"
"   sepallength  sepalwidth  petallength  petalwidth             name\n"
"0          5.0         2.0          3.5         1.0  Iris-versicolor\n"
"1          6.0         2.2          4.0         1.0  Iris-versicolor\n"
"2          6.2         2.2          4.5         1.5  Iris-versicolor\n"
"3          6.0         2.2          5.0         1.5   Iris-virginica\n"
"4          4.5         2.3          1.3         0.3      Iris-setosa"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:65
msgid ""
"多字段排序时，如果是升序降序不同，\\ ``ascending``\\ "
"参数可以传入一个列表，长度必须等同于排序的字段，它们的值都是boolean类型"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:76
msgid ""
">>> iris.sort(['sepalwidth', 'petallength'], ascending=[True, "
"False]).head(5)\n"
"   sepallength  sepalwidth  petallength  petalwidth             name\n"
"0          5.0         2.0          3.5         1.0  Iris-versicolor\n"
"1          6.0         2.2          5.0         1.5   Iris-virginica\n"
"2          6.2         2.2          4.5         1.5  Iris-versicolor\n"
"3          6.0         2.2          4.0         1.0  Iris-versicolor\n"
"4          6.3         2.3          4.4         1.3  Iris-versicolor"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:77
msgid "下面效果是一样的："
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:89
msgid ""
">>> iris.sort(['sepalwidth', -iris.petallength]).head(5)\n"
"   sepallength  sepalwidth  petallength  petalwidth             name\n"
"0          5.0         2.0          3.5         1.0  Iris-versicolor\n"
"1          6.0         2.2          5.0         1.5   Iris-virginica\n"
"2          6.2         2.2          4.5         1.5  Iris-versicolor\n"
"3          6.0         2.2          4.0         1.0  Iris-versicolor\n"
"4          6.3         2.3          4.4         1.3  Iris-versicolor"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:92
msgid ""
"由于 ODPS 要求排序必须指定个数，所以在 ODPS 后端执行时， 会通过 ``options.df.odps.sort.limit`` "
"指定排序个数，这个值默认是 10000， 如果要排序尽量多的数据，可以把这个值设到较大的值。不过注意，此时可能会导致 OOM。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:97
msgid "去重"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:99
msgid "去重在Collection上，用户可以调用distinct方法。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:108
msgid ""
">>> iris[['name']].distinct()\n"
"              name\n"
"0      Iris-setosa\n"
"1  Iris-versicolor\n"
"2   Iris-virginica"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:116
msgid ""
">>> iris.distinct('name')\n"
"              name\n"
"0      Iris-setosa\n"
"1  Iris-versicolor\n"
"2   Iris-virginica"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:124
msgid ""
">>> iris.distinct('name', 'sepallength').head(3)\n"
"          name  sepallength\n"
"0  Iris-setosa          4.3\n"
"1  Iris-setosa          4.4\n"
"2  Iris-setosa          4.5"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:125
msgid "在Sequence上，用户可以调用unique，但是记住，调用unique的Sequence不能用在列选择中。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:135
msgid ""
">>> iris.name.unique()\n"
"              name\n"
"0      Iris-setosa\n"
"1  Iris-versicolor\n"
"2   Iris-virginica"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:136
msgid "下面的代码是错误的用法。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:142
msgid ">>> iris[iris.name, iris.name.unique()]  # 错误的"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:144
msgid "采样"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:147
msgid "要对一个 collection 的数据采样，可以调用 ``sample`` 方法。PyODPS 支持四种采样方式。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:150
msgid ""
"除了按份数采样外，其余方法如果要在 ODPS DataFrame 上执行，需要 Project 支持 XFlow，否则，这些方法只能在 "
"Pandas DataFrame 后端上执行。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:153
msgid "按份数采样"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:155
msgid "在这种采样方式下，数据被分为 ``parts`` 份，可选择选取的份数序号。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:163
msgid ""
">>> iris.sample(parts=10)  # 分成10份，默认取第0份\n"
">>> iris.sample(parts=10, i=0)  # 手动指定取第0份\n"
">>> iris.sample(parts=10, i=[2, 5])   # 分成10份，取第2和第5份\n"
">>> iris.sample(parts=10, columns=['name', 'sepalwidth'])  # "
"根据name和sepalwidth的值做采样"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:164
msgid "按比例 / 条数采样"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:166
msgid "在这种采样方式下，用户指定需要采样的数据条数或采样比例。指定 ``replace`` 参数为 True 可启用放回采样。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:172
msgid ""
">>> iris.sample(n=100)  # 选取100条数据\n"
">>> iris.sample(frac=0.3)  # 采样30%的数据"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:173
msgid "按权重列采样"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:175
msgid "在这种采样方式下，用户指定权重列和数据条数 / 采样比例。指定 ``replace`` 参数为 True 可启用放回采样。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:181
msgid ""
">>> iris.sample(n=100, weights='sepal_length')\n"
">>> iris.sample(n=100, weights='sepal_width', replace=True)"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:182
msgid "分层采样"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:184
msgid ""
"在这种采样方式下，用户指定用于分层的标签列，同时为需要采样的每个标签指定采样比例（ ``frac`` 参数）或条数 （ ``n`` "
"参数）。暂不支持放回采样。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:191
msgid ""
">>> iris.sample(strata='category', n={'Iris Setosa': 10, 'Iris "
"Versicolour': 10})\n"
">>> iris.sample(strata='category', frac={'Iris Setosa': 0.5, 'Iris "
"Versicolour': 0.4})"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:193
msgid "数据缩放"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:195
msgid "DataFrame 支持通过最大/最小值或平均值/标准差对数据进行缩放。例如，对数据"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:206
msgid ""
"    name  id  fid\n"
"0  name1   4  5.3\n"
"1  name2   2  3.5\n"
"2  name2   3  1.5\n"
"3  name1   4  4.2\n"
"4  name1   3  2.2\n"
"5  name1   3  4.1"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:207
msgid "使用 min_max_scale 方法进行归一化："
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:219
msgid ""
">>> df.min_max_scale(columns=['fid'])\n"
"    name  id       fid\n"
"0  name1   4  1.000000\n"
"1  name2   2  0.526316\n"
"2  name2   3  0.000000\n"
"3  name1   4  0.710526\n"
"4  name1   3  0.184211\n"
"5  name1   3  0.684211"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:220
msgid ""
"min_max_scale 还支持使用 feature_range 参数指定输出值的范围，例如，如果我们需要使输出值在 (-1, 1) "
"范围内，可使用"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:233
msgid ""
">>> df.min_max_scale(columns=['fid'], feature_range=(-1, 1))\n"
"    name  id       fid\n"
"0  name1   4  1.000000\n"
"1  name2   2  0.052632\n"
"2  name2   3 -1.000000\n"
"3  name1   4  0.421053\n"
"4  name1   3 -0.631579\n"
"5  name1   3  0.368421"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:234
msgid ""
"如果需要保留原始值，可以使用 preserve 参数。此时，缩放后的数据将会以新增列的形式追加到数据中， "
"列名默认为原列名追加“_scaled”后缀，该后缀可使用 suffix 参数更改。例如，"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:247
msgid ""
">>> df.min_max_scale(columns=['fid'], preserve=True)\n"
"    name  id  fid  fid_scaled\n"
"0  name1   4  5.3    1.000000\n"
"1  name2   2  3.5    0.526316\n"
"2  name2   3  1.5    0.000000\n"
"3  name1   4  4.2    0.710526\n"
"4  name1   3  2.2    0.184211\n"
"5  name1   3  4.1    0.684211"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:248
msgid "min_max_scale 也支持使用 group 参数指定一个或多个分组列，在分组列中分别取最值进行缩放。例如，"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:260
msgid ""
">>> df.min_max_scale(columns=['fid'], group=['name'])\n"
"    name  id       fid\n"
"0  name1   4  1.000000\n"
"1  name1   4  0.645161\n"
"2  name1   3  0.000000\n"
"3  name1   3  0.612903\n"
"4  name2   2  1.000000\n"
"5  name2   3  0.000000"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:261
msgid "可见结果中，name1 和 name2 两组均按组中的最值进行了缩放。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:263
msgid "std_scale 可依照标准正态分布对数据进行调整。例如，"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:275
msgid ""
">>> df.std_scale(columns=['fid'])\n"
"    name  id       fid\n"
"0  name1   4  1.436467\n"
"1  name2   2  0.026118\n"
"2  name2   3 -1.540938\n"
"3  name1   4  0.574587\n"
"4  name1   3 -0.992468\n"
"5  name1   3  0.496234"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:276
msgid "std_scale 同样支持 preserve 参数保留原始列以及使用 group 进行分组，具体请参考 min_max_scale，此处不再赘述。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:279
msgid "空值处理"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:281
msgid "DataFrame 支持筛去空值以及填充空值的功能。例如，对数据"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:293
msgid ""
"   id   name   f1   f2   f3   f4\n"
"0   0  name1  1.0  NaN  3.0  4.0\n"
"1   1  name1  2.0  NaN  NaN  1.0\n"
"2   2  name1  3.0  4.0  1.0  NaN\n"
"3   3  name1  NaN  1.0  2.0  3.0\n"
"4   4  name1  1.0  NaN  3.0  4.0\n"
"5   5  name1  1.0  2.0  3.0  4.0\n"
"6   6  name1  NaN  NaN  NaN  NaN"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:294
msgid "使用 dropna 可删除 subset 中包含空值的行："
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:301
msgid ""
">>> df.dropna(subset=['f1', 'f2', 'f3', 'f4'])\n"
"   id   name   f1   f2   f3   f4\n"
"0   5  name1  1.0  2.0  3.0  4.0"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:302
msgid "如果行中包含非空值则不删除，可以使用 how='all'："
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:314
msgid ""
">>> df.dropna(how='all', subset=['f1', 'f2', 'f3', 'f4'])\n"
"   id   name   f1   f2   f3   f4\n"
"0   0  name1  1.0  NaN  3.0  4.0\n"
"1   1  name1  2.0  NaN  NaN  1.0\n"
"2   2  name1  3.0  4.0  1.0  NaN\n"
"3   3  name1  NaN  1.0  2.0  3.0\n"
"4   4  name1  1.0  NaN  3.0  4.0\n"
"5   5  name1  1.0  2.0  3.0  4.0"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:315
msgid "你也可以使用 thresh 参数来指定行中至少要有多少个非空值。例如："
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:326
msgid ""
">>> df.dropna(thresh=3, subset=['f1', 'f2', 'f3', 'f4'])\n"
"   id   name   f1   f2   f3   f4\n"
"0   0  name1  1.0  NaN  3.0  4.0\n"
"2   2  name1  3.0  4.0  1.0  NaN\n"
"3   3  name1  NaN  1.0  2.0  3.0\n"
"4   4  name1  1.0  NaN  3.0  4.0\n"
"5   5  name1  1.0  2.0  3.0  4.0"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:327
msgid "使用 fillna 可使用常数或已有的列填充未知值。下面给出了使用常数填充的例子："
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:340
msgid ""
">>> df.fillna(100, subset=['f1', 'f2', 'f3', 'f4'])\n"
"   id   name     f1     f2     f3     f4\n"
"0   0  name1    1.0  100.0    3.0    4.0\n"
"1   1  name1    2.0  100.0  100.0    1.0\n"
"2   2  name1    3.0    4.0    1.0  100.0\n"
"3   3  name1  100.0    1.0    2.0    3.0\n"
"4   4  name1    1.0  100.0    3.0    4.0\n"
"5   5  name1    1.0    2.0    3.0    4.0\n"
"6   6  name1  100.0  100.0  100.0  100.0"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:341
msgid "你也可以使用一个已有的列来填充未知值。例如："
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:354
msgid ""
">>> df.fillna(df.f2, subset=['f1', 'f2', 'f3', 'f4'])\n"
"   id   name   f1   f2   f3   f4\n"
"0   0  name1  1.0  NaN  3.0  4.0\n"
"1   1  name1  2.0  NaN  NaN  1.0\n"
"2   2  name1  3.0  4.0  1.0  4.0\n"
"3   3  name1  1.0  1.0  2.0  3.0\n"
"4   4  name1  1.0  NaN  3.0  4.0\n"
"5   5  name1  1.0  2.0  3.0  4.0\n"
"6   6  name1  NaN  NaN  NaN  NaN"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:355
msgid "特别地，DataFrame 提供了向前 / 向后填充的功能。通过指定 method 参数为下列值可以达到目的："
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:358
msgid "取值"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:358
msgid "含义"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:360
msgid "bfill / backfill"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:360
msgid "向前填充"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:361
msgid "ffill / pad"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:361
msgid "向后填充"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:364
msgid "例如："
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:386
msgid ""
">>> df.fillna(method='bfill', subset=['f1', 'f2', 'f3', 'f4'])\n"
"   id   name   f1   f2   f3   f4\n"
"0   0  name1  1.0  3.0  3.0  4.0\n"
"1   1  name1  2.0  1.0  1.0  1.0\n"
"2   2  name1  3.0  4.0  1.0  NaN\n"
"3   3  name1  1.0  1.0  2.0  3.0\n"
"4   4  name1  1.0  3.0  3.0  4.0\n"
"5   5  name1  1.0  2.0  3.0  4.0\n"
"6   6  name1  NaN  NaN  NaN  NaN\n"
">>> df.fillna(method='ffill', subset=['f1', 'f2', 'f3', 'f4'])\n"
"   id   name   f1   f2   f3   f4\n"
"0   0  name1  1.0  1.0  3.0  4.0\n"
"1   1  name1  2.0  2.0  2.0  1.0\n"
"2   2  name1  3.0  4.0  1.0  1.0\n"
"3   3  name1  NaN  1.0  2.0  3.0\n"
"4   4  name1  1.0  1.0  3.0  4.0\n"
"5   5  name1  1.0  2.0  3.0  4.0\n"
"6   6  name1  NaN  NaN  NaN  NaN"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:387
msgid ""
"你也可以使用 ffill / bfill 函数来简化代码。ffill 等价于 fillna(method='ffill')， bfill 等价于 "
"fillna(method='bfill')"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:391
msgid "对所有行/列调用自定义函数"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:396
msgid "对一行数据使用自定义函数"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:398
msgid "要对一行数据使用自定义函数，可以使用 apply 方法，axis 参数必须为 1，表示在行上操作。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:400
msgid "apply 的自定义函数接收一个参数，为上一步 Collection 的一行数据，用户可以通过属性、或者偏移取得一个字段的数据。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:409
msgid ""
">>> iris.apply(lambda row: row.sepallength + row.sepalwidth, axis=1, "
"reduce=True, types='float').rename('sepaladd').head(3)\n"
"   sepaladd\n"
"0       8.6\n"
"1       7.9\n"
"2       7.9"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:410
msgid ""
"``reduce``\\ 为 True 时，表示返回结果为Sequence，否则返回结果为Collection。 ``names``\\ 和 "
"``types``\\ 参数分别指定返回的Sequence或Collection的字段名和类型。 如果类型不指定，将会默认为string类型。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:414
msgid "在 apply 的自定义函数中，reduce 为 False 时，也可以使用 ``yield``\\ 关键字来返回多行结果。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:427
msgid ""
">>> iris.count()\n"
"150\n"
">>>\n"
">>> def handle(row):\n"
">>>     yield row.sepallength - row.sepalwidth, row.sepallength + "
"row.sepalwidth\n"
">>>     yield row.petallength - row.petalwidth, row.petallength + "
"row.petalwidth\n"
">>>\n"
">>> iris.apply(handle, axis=1, names=['iris_add', 'iris_sub'], "
"types=['float', 'float']).count()\n"
"300"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:428
msgid "我们也可以在函数上来注释返回的字段和类型，这样就不需要在函数调用时再指定。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:442
msgid ""
">>> from odps.df import output\n"
">>>\n"
">>> @output(['iris_add', 'iris_sub'], ['float', 'float'])\n"
">>> def handle(row):\n"
">>>     yield row.sepallength - row.sepalwidth, row.sepallength + "
"row.sepalwidth\n"
">>>     yield row.petallength - row.petalwidth, row.petallength + "
"row.petalwidth\n"
">>>\n"
">>> iris.apply(handle, axis=1).count()\n"
"300"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:443
msgid "也可以使用 map-only 的 map_reduce，和 axis=1 的apply操作是等价的。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:449
msgid ""
">>> iris.map_reduce(mapper=handle).count()\n"
"300"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:450
msgid "如果想调用 ODPS 上已经存在的 UDTF，则函数指定为函数名即可。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:455
msgid ""
">>> iris['name', 'sepallength'].apply('your_func', axis=1, "
"names=['name2', 'sepallength2'], types=['string', 'float'])"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:456
msgid ""
"使用 apply 对行操作，且 ``reduce``\\ 为 False 时，可以使用 :ref:`dflateralview` "
"与已有的行结合，用于后续聚合等操作。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:468
msgid ""
">>> from odps.df import output\n"
">>>\n"
">>> @output(['iris_add', 'iris_sub'], ['float', 'float'])\n"
">>> def handle(row):\n"
">>>     yield row.sepallength - row.sepalwidth, row.sepallength + "
"row.sepalwidth\n"
">>>     yield row.petallength - row.petalwidth, row.petallength + "
"row.petalwidth\n"
">>>\n"
">>> iris[iris.category, iris.apply(handle, axis=1)]"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:470
msgid "对所有列调用自定义聚合"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:472
msgid "调用apply方法，当我们不指定axis，或者axis为0的时候，我们可以通过传入一个自定义聚合类来对所有sequence进行聚合操作。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:474
msgid ""
"class Agg(object):\n"
"\n"
"    def buffer(self):\n"
"        return [0.0, 0]\n"
"\n"
"    def __call__(self, buffer, val):\n"
"        buffer[0] += val\n"
"        buffer[1] += 1\n"
"\n"
"    def merge(self, buffer, pbuffer):\n"
"        buffer[0] += pbuffer[0]\n"
"        buffer[1] += pbuffer[1]\n"
"\n"
"    def getvalue(self, buffer):\n"
"        if buffer[1] == 0:\n"
"            return 0.0\n"
"        return buffer[0] / buffer[1]"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:499
msgid ""
">>> iris.exclude('name').apply(Agg)\n"
"   sepallength_aggregation  sepalwidth_aggregation  "
"petallength_aggregation  petalwidth_aggregation\n"
"0                 5.843333                   3.054                 "
"3.758667                1.198667"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:501
msgid "目前，受限于 Python UDF，自定义函数无法支持将 list / dict 类型作为初始输入或最终输出结果。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:504
#: ../../source/df-sort-distinct-apply.rst:668
msgid "引用资源"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:506
msgid ""
"类似于对 :ref:`map <map>` "
"方法的resources参数，每个resource可以是ODPS上的资源（表资源或文件资源），或者引用一个collection作为资源。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:508
msgid ""
"对于axis为1，也就是在行上操作，我们需要写一个函数闭包或者callable的类。 而对于列上的聚合操作，我们只需在 "
"\\_\\_init\\_\\_ 函数里读取资源即可。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:535
msgid ""
">>> words_df\n"
"                     sentence\n"
"0                 Hello World\n"
"1                Hello Python\n"
"2  Life is short I use Python\n"
">>>\n"
">>> import pandas as pd\n"
">>> stop_words = DataFrame(pd.DataFrame({'stops': ['is', 'a', 'I']}))\n"
">>>\n"
">>> @output(['sentence'], ['string'])\n"
">>> def filter_stops(resources):\n"
">>>     stop_words = set([r[0] for r in resources[0]])\n"
">>>     def h(row):\n"
">>>         return ' '.join(w for w in row[0].split() if w not in "
"stop_words),\n"
">>>     return h\n"
">>>\n"
">>> words_df.apply(filter_stops, axis=1, resources=[stop_words])\n"
"                sentence\n"
"0            Hello World\n"
"1           Hello Python\n"
"2  Life short use Python"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:536
msgid "可以看到这里的stop_words是存放于本地，但在真正执行时会被上传到ODPS作为资源引用。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:540
#: ../../source/df-sort-distinct-apply.rst:714
msgid "使用第三方Python库"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:542
#: ../../source/df-sort-distinct-apply.rst:717
msgid "使用方法类似 :ref:`map中使用第三方Python库 <third_party_library>` 。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:544
#: ../../source/df-sort-distinct-apply.rst:719
msgid "可以在全局指定使用的库："
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:550
#: ../../source/df-sort-distinct-apply.rst:726
msgid ""
">>> from odps import options\n"
">>> options.df.libraries = ['six.whl', 'python_dateutil.whl']"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:551
#: ../../source/df-sort-distinct-apply.rst:727
msgid "或者在立即执行的方法中，局部指定："
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:556
msgid ""
">>> df.apply(my_func, axis=1).to_pandas(libraries=['six.whl', "
"'python_dateutil.whl'])"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:558
#: ../../source/df-sort-distinct-apply.rst:735
msgid ""
"由于字节码定义的差异，Python 3 下使用新语言特性（例如 ``yield from`` ）时，代码在使用 Python 2.7 的 ODPS"
" Worker 上执行时会发生错误。因而建议在 Python 3 下使用 MapReduce API 编写生产作业前，先确认相关代码是否能正常 "
"执行。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:565
msgid "MapReduce API"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:568
msgid ""
"PyODPS DataFrame也支持MapReduce "
"API，用户可以分别编写map和reduce函数（map_reduce可以只有mapper或者reducer过程）。 "
"我们来看个简单的wordcount的例子。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:600
msgid ""
">>> def mapper(row):\n"
">>>     for word in row[0].split():\n"
">>>         yield word.lower(), 1\n"
">>>\n"
">>> def reducer(keys):\n"
">>>     cnt = [0]\n"
">>>     def h(row, done):  # done表示这个key已经迭代结束\n"
">>>         cnt[0] += row[1]\n"
">>>         if done:\n"
">>>             yield keys[0], cnt[0]\n"
">>>     return h\n"
">>>\n"
">>> words_df.map_reduce(mapper, reducer, group=['word', ],\n"
">>>                     mapper_output_names=['word', 'cnt'],\n"
">>>                     mapper_output_types=['string', 'int'],\n"
">>>                     reducer_output_names=['word', 'cnt'],\n"
">>>                     reducer_output_types=['string', 'int'])\n"
"     word  cnt\n"
"0   hello    2\n"
"1       i    1\n"
"2      is    1\n"
"3    life    1\n"
"4  python    2\n"
"5   short    1\n"
"6     use    1\n"
"7   world    1"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:601
msgid "group参数用来指定reduce按哪些字段做分组，如果不指定，会按全部字段做分组。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:603
msgid ""
"其中对于reducer来说，会稍微有些不同。它需要接收聚合的keys初始化，并能继续处理按这些keys聚合的每行数据。 "
"第2个参数表示这些keys相关的所有行是不是都迭代完成。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:606
msgid "这里写成函数闭包的方式，主要为了方便，当然我们也能写成callable的类。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:608
msgid ""
"class reducer(object):\n"
"    def __init__(self, keys):\n"
"        self.cnt = 0\n"
"\n"
"    def __call__(self, row, done):  # done表示这个key已经迭代结束\n"
"        self.cnt += row.cnt\n"
"        if done:\n"
"            yield row.word, self.cnt"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:619
msgid "使用 ``output``\\ 来注释会让代码更简单些。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:649
msgid ""
">>> from odps.df import output\n"
">>>\n"
">>> @output(['word', 'cnt'], ['string', 'int'])\n"
">>> def mapper(row):\n"
">>>     for word in row[0].split():\n"
">>>         yield word.lower(), 1\n"
">>>\n"
">>> @output(['word', 'cnt'], ['string', 'int'])\n"
">>> def reducer(keys):\n"
">>>     cnt = [0]\n"
">>>     def h(row, done):  # done表示这个key已经迭代结束\n"
">>>         cnt[0] += row.cnt\n"
">>>         if done:\n"
">>>             yield keys.word, cnt[0]\n"
">>>     return h\n"
">>>\n"
">>> words_df.map_reduce(mapper, reducer, group='word')\n"
"     word  cnt\n"
"0   hello    2\n"
"1       i    1\n"
"2      is    1\n"
"3    life    1\n"
"4  python    2\n"
"5   short    1\n"
"6     use    1\n"
"7   world    1"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:650
msgid ""
"有时候我们在迭代的时候需要按某些列排序，则可以使用 ``sort``\\ 参数，来指定按哪些列排序，升序降序则通过 ``ascending``\\"
" 参数指定。 ``ascending`` 参数可以是一个bool值，表示所有的 ``sort``\\ 字段是相同升序或降序， "
"也可以是一个列表，长度必须和 ``sort``\\ 字段长度相同。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:656
msgid "指定combiner"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:658
msgid ""
"combiner表示在map_reduce "
"API里表示在mapper端，就先对数据进行聚合操作，它的用法和reducer是完全一致的，但不能引用资源。 "
"并且，combiner的输出的字段名和字段类型必须和mapper完全一致。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:661
msgid "上面的例子，我们就可以使用reducer作为combiner来先在mapper端对数据做初步的聚合，减少shuffle出去的数据量。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:666
msgid ">>> words_df.map_reduce(mapper, reducer, combiner=reducer, group='word')"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:670
msgid "在MapReduce API里，我们能分别指定mapper和reducer所要引用的资源。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:672
msgid "如下面的例子，我们对mapper里的单词做停词过滤，在reducer里对白名单的单词数量加5。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:712
msgid ""
">>> white_list_file = o.create_resource('pyodps_white_list_words', "
"'file', file_obj='Python\\nWorld')\n"
">>>\n"
">>> @output(['word', 'cnt'], ['string', 'int'])\n"
">>> def mapper(resources):\n"
">>>     stop_words = set(r[0].strip() for r in resources[0])\n"
">>>     def h(row):\n"
">>>         for word in row[0].split():\n"
">>>             if word not in stop_words:\n"
">>>                 yield word, 1\n"
">>>     return h\n"
">>>\n"
">>> @output(['word', 'cnt'], ['string', 'int'])\n"
">>> def reducer(resources):\n"
">>>     d = dict()\n"
">>>     d['white_list'] = set(word.strip() for word in resources[0])\n"
">>>     d['cnt'] = 0\n"
">>>     def inner(keys):\n"
">>>         d['cnt'] = 0\n"
">>>         def h(row, done):\n"
">>>             d['cnt'] += row.cnt\n"
">>>             if done:\n"
">>>                 if row.word in d['white_list']:\n"
">>>                     d['cnt'] += 5\n"
">>>                 yield keys.word, d['cnt']\n"
">>>         return h\n"
">>>     return inner\n"
">>>\n"
">>> words_df.map_reduce(mapper, reducer, group='word',\n"
">>>                     mapper_resources=[stop_words], "
"reducer_resources=[white_list_file])\n"
"     word  cnt\n"
"0   hello    2\n"
"1    life    1\n"
"2  python    7\n"
"3   world    6\n"
"4   short    1\n"
"5     use    1"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:733
msgid ""
">>> df.map_reduce(mapper=my_mapper, reducer=my_reducer, "
"group='key').execute(libraries=['six.whl', 'python_dateutil.whl'])"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:741
msgid "重排数据"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:743
msgid "有时候我们的数据在集群上分布可能是不均匀的，我们需要对数据重排。调用 ``reshuffle`` 接口即可。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:750
msgid ">>> df1 = df.reshuffle()"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:751
msgid "默认会按随机数做哈希来分布。也可以指定按那些列做分布，且可以指定重排后的排序顺序。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:758
msgid ">>> df1.reshuffle('name', sort='id', ascending=False)"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:760
msgid "布隆过滤器"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:763
msgid "PyODPS DataFrame提供了 ``bloom_filter`` 接口来进行布隆过滤器的计算。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:765
msgid ""
"给定某个collection，和它的某个列计算的sequence1，我们能对另外一个sequence2进行布隆过滤，sequence1不在sequence2中的一定会过滤，"
" 但可能不能完全过滤掉不存在于sequence2中的数据，这也是一种近似的方法。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:768
msgid "这样的好处是能快速对collection进行快速过滤一些无用数据。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:770
msgid ""
"这在大规模join的时候，一边数据量远大过另一边数据，而大部分并不会join上的场景很有用。 "
"比如，我们在join用户的浏览数据和交易数据时，用户的浏览大部分不会带来交易，我们可以利用交易数据先对浏览数据进行布隆过滤， "
"然后再join能很好提升性能。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:791
msgid ""
">>> df1 = DataFrame(pd.DataFrame({'a': ['name1', 'name2', 'name3', "
"'name1'], 'b': [1, 2, 3, 4]}))\n"
">>> df1\n"
"       a  b\n"
"0  name1  1\n"
"1  name2  2\n"
"2  name3  3\n"
"3  name1  4\n"
">>> df2 = DataFrame(pd.DataFrame({'a': ['name1']}))\n"
">>> df2\n"
"       a\n"
"0  name1\n"
">>> df1.bloom_filter('a', df2.a) # 这里第0个参数可以是个计算表达式如: df1.a + '1'\n"
"       a  b\n"
"0  name1  1\n"
"1  name1  4"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:792
msgid "这里由于数据量很小，df1中的a为name2和name3的行都被正确过滤掉了，当数据量很大的时候，可能会有一定的数据不能被过滤。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:794
msgid "如之前提的join场景中，少量不能过滤并不能并不会影响正确性，但能较大提升join的性能。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:796
msgid ""
"我们可以传入 ``capacity`` 和 ``error_rate`` 来设置数据的量以及错误率，默认值是 ``3000`` 和 "
"``0.01``。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:799
msgid "要注意，调大 ``capacity`` 或者减小 ``error_rate`` 会增加内存的使用，所以应当根据实际情况选择一个合理的值。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:805
msgid "透视表（pivot_table）"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:807
msgid "PyODPS DataFrame提供透视表的功能。我们通过几个例子来看使用。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:824
msgid ""
">>> df\n"
"     A    B      C  D  E\n"
"0  foo  one  small  1  3\n"
"1  foo  one  large  2  4\n"
"2  foo  one  large  2  5\n"
"3  foo  two  small  3  6\n"
"4  foo  two  small  3  4\n"
"5  bar  one  large  4  5\n"
"6  bar  one  small  5  3\n"
"7  bar  two  small  6  2\n"
"8  bar  two  large  7  1"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:825
msgid "最简单的透视表必须提供一个 ``rows`` 参数，表示按一个或者多个字段做取平均值的操作。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:833
msgid ""
">>> df['A', 'D', 'E'].pivot_table(rows='A')\n"
"     A  D_mean  E_mean\n"
"0  bar     5.5    2.75\n"
"1  foo     2.2    4.40"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:834
msgid "rows可以提供多个，表示按多个字段做聚合。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:847
msgid ""
">>> df.pivot_table(rows=['A', 'B', 'C'])\n"
"     A    B      C  D_mean  E_mean\n"
"0  bar  one  large     4.0     5.0\n"
"1  bar  one  small     5.0     3.0\n"
"2  bar  two  large     7.0     1.0\n"
"3  bar  two  small     6.0     2.0\n"
"4  foo  one  large     2.0     4.5\n"
"5  foo  one  small     1.0     3.0\n"
"6  foo  two  small     3.0     5.0"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:848
msgid "我们可以指定 ``values`` 来显示指定要计算的列。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:858
msgid ""
">>> df.pivot_table(rows=['A', 'B'], values='D')\n"
"     A    B    D_mean\n"
"0  bar  one  4.500000\n"
"1  bar  two  6.500000\n"
"2  foo  one  1.666667\n"
"3  foo  two  3.000000"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:859
msgid "计算值列时，默认会计算平均值，用户可以指定一个或者多个聚合函数。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:869
msgid ""
">>> df.pivot_table(rows=['A', 'B'], values=['D'], aggfunc=['mean', "
"'count', 'sum'])\n"
"     A    B    D_mean  D_count  D_sum\n"
"0  bar  one  4.500000        2      9\n"
"1  bar  two  6.500000        2     13\n"
"2  foo  one  1.666667        3      5\n"
"3  foo  two  3.000000        2      6"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:870
msgid "我们也可以把原始数据的某一列的值，作为新的collection的列。 **这也是透视表最强大的地方。**"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:880
msgid ""
">>> df.pivot_table(rows=['A', 'B'], values='D', columns='C')\n"
"     A    B  large_D_mean  small_D_mean\n"
"0  bar  one           4.0           5.0\n"
"1  bar  two           7.0           6.0\n"
"2  foo  one           2.0           1.0\n"
"3  foo  two           NaN           3.0"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:881
msgid "我们可以提供 ``fill_value`` 来填充空值。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:892
msgid ""
">>> df.pivot_table(rows=['A', 'B'], values='D', columns='C', "
"fill_value=0)\n"
"     A    B  large_D_mean  small_D_mean\n"
"0  bar  one             4             5\n"
"1  bar  two             7             6\n"
"2  foo  one             2             1\n"
"3  foo  two             0             3"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:894
msgid "Key-Value 字符串转换"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:896
msgid "DataFrame 提供了将 Key-Value 对展开为列，以及将普通列转换为 Key-Value 列的功能。"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:898
msgid "我们的数据为"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:908
msgid ""
">>> df\n"
"    name               kv\n"
"0  name1  k1=1,k2=3,k5=10\n"
"1  name1    k1=7.1,k7=8.2\n"
"2  name2    k2=1.2,k3=1.5\n"
"3  name2      k9=1.1,k2=1"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:909
msgid "可以通过 extract_kv 方法将 Key-Value 字段展开："
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:919
msgid ""
">>> df.extract_kv(columns=['kv'], kv_delim='=', item_delim=',')\n"
"   name   kv_k1  kv_k2  kv_k3  kv_k5  kv_k7  kv_k9\n"
"0  name1    1.0    3.0    NaN   10.0    NaN    NaN\n"
"1  name1    7.0    NaN    NaN    NaN    8.2    NaN\n"
"2  name2    NaN    1.2    1.5    NaN    NaN    NaN\n"
"3  name2    NaN    1.0    NaN    NaN    NaN    1.1"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:920
msgid ""
"其中，需要展开的字段名由 columns 指定，Key 和 Value 之间的分隔符，以及 Key-Value 对之间的分隔符分别由 "
"kv_delim 和 item_delim 这两个参数指定，默认分别为半角冒号和半角逗号。输出的字段名为原字段名和 Key "
"值的组合，通过“_”相连。缺失值默认为 None，可通过 ``fill_value`` 选择需要填充的值。例如，相同的 df，"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:932
msgid ""
">>> df.extract_kv(columns=['kv'], kv_delim='=', fill_value=0)\n"
"   name   kv_k1  kv_k2  kv_k3  kv_k5  kv_k7  kv_k9\n"
"0  name1    1.0    3.0    0.0   10.0    0.0    0.0\n"
"1  name1    7.0    0.0    0.0    0.0    8.2    0.0\n"
"2  name2    0.0    1.2    1.5    0.0    0.0    0.0\n"
"3  name2    0.0    1.0    0.0    0.0    0.0    1.1"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:933
msgid "DataFrame 也支持将多列数据转换为一个 Key-Value 列。例如，"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:943
msgid ""
">>> df\n"
"   name    k1   k2   k3    k5   k7   k9\n"
"0  name1  1.0  3.0  NaN  10.0  NaN  NaN\n"
"1  name1  7.0  NaN  NaN   NaN  8.2  NaN\n"
"2  name2  NaN  1.2  1.5   NaN  NaN  NaN\n"
"3  name2  NaN  1.0  NaN   NaN  NaN  1.1"
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:944
msgid "可通过 to_kv 方法转换为 Key-Value 表示的格式："
msgstr ""

#: ../../source/df-sort-distinct-apply.rst:953
msgid ""
">>> df.to_kv(columns=['k1', 'k2', 'k3', 'k5', 'k7', 'k9'], kv_delim='=')\n"
"    name               kv\n"
"0  name1  k1=1,k2=3,k5=10\n"
"1  name1    k1=7.1,k7=8.2\n"
"2  name2    k2=1.2,k3=1.5\n"
"3  name2      k9=1.1,k2=1"
msgstr ""

